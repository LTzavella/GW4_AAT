---
title: The affective priming paradigm as an implicit measure of food attitudes and
  related choice behaviour
author:
- address: CUBRIC, Maindy Road, Cardiff, CF24 4HQ
  affiliation: '1'
  corresponding: yes
  email: tzavellal@cardiff.ac.uk
  name: Loukia Tzavella
- affiliation: '1'
  name: Leah Maizey
- affiliation: '1'
  name: Andrew D Lawrence
- affiliation: '1'
  name: Christopher D Chambers
affiliation:
- id: '1'
  institution: CUBRIC, School of Psychology, Cardiff University, UK
output: papaja::apa6_pdf
bibliography: rr_references.bib
classoption: "man"
documentclass: "apa6"
draft: no
figurelist: no
floatsintext: yes
footnotelist: no
header-includes:
- \usepackage{xcolor}
- \usepackage{multicol}
- \usepackage{enumitem}
- \usepackage[font=footnotesize,labelfont=bf]{caption}
- \usepackage{float}
- \usepackage{amsmath}
keywords: Affective priming; Foods; Liking; Choice; Healthiness
linenumbers: no
mask: no
authornote: |
  This is a draft manuscript in preparation for submission to Psychonomic Bulletin & Review, as a Stage 2 Registered Report.
  Data collection is still underway and therefore full exploratory analyses and certain sections have been omitted at this stage (e.g. Discussion). Supplementary Material not submitted, as it has already been reviewed for the S1 submission. 
shorttitle: Affective food priming
tablelist: no
#abstract: |
  
wordcount: X
---

```{r load_pkgs, message=FALSE, include=FALSE}
# List of packages required for this analysis
pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "papaja", "devtools", "kableExtra", "devtools", "BFDA")
# Check if packages are not installed and assign the
# names of the packages not installed to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg))
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
if("BFDA" %in% new.pkg) {install_github("nicebread/BFDA", subdir="package")}

# Load packages 
require(dplyr)
require(ggplot2)
require(knitr)
require(bookdown)
require(papaja)
require(devtools)
require(kableExtra)
```

#Introduction {#introduction}

There is an emerging need for a greater understanding of attitudes towards foods that may drive unhealthy eating behaviours, such as overeating. Attitudes reflect “object-evaluation associations” that can be retrieved from memory and influence behaviour towards the attitude object [@klauer_affective_2003]. For example, individuals may respond positively to a food that contains intrinsically rewarding ingredients (e.g., sugar, fat), with the positive evaluation automatically activated by the learned association between reward and consumption. Evaluations of foods arise from both affective and cognitive components of attitudes [@marty_hedonic-_2017]. The affective component reflects an individual’s hedonic reaction to the sensory properties of foods, commonly referred to as food liking, which is a central determinant of dietary choice [@eertmans_food_2001]. The cognitive component may involve thoughts about the nutritional value of a food item and potential health consequences [@Trendel2015]. In this study, we examine the methodological validity of an implicit measure of attitudes - the affective priming paradigm[APP; @fazio_automatic_1986; @fazio_implicit_2003; @hermans_time_2001; @klauer_affective_2003] and the extent to which priming measures are sensitive to affective (i.e., liking) and cognitive (i.e., healthiness) components of food attitudes. The association between priming measures and food choice behaviour would also be investigated.  
The interplay between affective and cognitive components of attitudes may be paramount to the understanding of eating behaviours, including food choices. Appetitive foods and their cues, such as sight or smell, can induce positive affective reactions [@blechert_food-pics:_2014] and activate the brain’s reward circuits associated with “wanting” and “liking” [@berridge_tempted_2010]. In food-rich societies, where high-calorie foods are heavily promoted, such cue-evoked positive reactions are frequent and can drive impulsive food choices [@zoltak_attention_2018] that likely contribute to overeating and other unhealthy eating behaviours [@berridge_tempted_2010; @lawrence_nucleus_2012; @sato_unconscious_2016]. These impulsive food choices are not guided by deliberate processes, such as the consideration of consequences [@veling_training_2017-3]. Cognitive components of attitudes include social norms and individual beliefs about the attitude object (i.e., foods), such as nutrition and health consequences and should be considered as determinants of eating behaviours [@eertmans_food_2001]. Interestingly, cognitive and affective components of attitudes can interact, as implicit measures can be influenced by various sources of valence, such as caloric content, economic cost and effects on one’s health [@verhulst_determinants_2006]. For example, unhealthy foods can be perceived to be tastier than healthy foods and chosen for consumption more frequently, even if individuals are not consciously aware of the association between healthiness and tastiness [@ackermann_contribution_2014].  
The APP has been previously applied to the food domain as an implicit, or indirect, measure of attitudes [e.g., @lamote_exploration_2004; @roefs_at_2005]. The current study employed a variant of the APP where attitude objects are presented as primes and are unrelated to the primary task of identifying the evaluative connotation of target words presented after the primes [@fazio_implicit_2003]. Participants were asked to perform an evaluative categorisation task, identifying target words as either positive or negative when preceded by either most liked (i.e., positive) or least liked (i.e., negative) food primes (see Figures 1 and 2). Here, the main outcome of interest is the affective priming effect, which manifests as faster responses (and/or lower error rates) on affectively congruent (i.e., most liked food-positive target or least liked food-negative target) than incongruent trials (i.e., most liked food-negative target or least liked food-positive target). Affective priming effects can be explained by response competition/facilitation processes [@fazio_implicit_2003] and in the food domain they are often utilised as a measure of liking or preferences. We posit that such priming measures may be influenced by both affective and cognitive components of attitudes and their association with food choice behaviour should be examined further.  
The APP in the food domain has been shown to capture the evaluation of foods (i.e., liking) through observed priming effects for both reaction times and error rates [@lamote_exploration_2004], even when attitudes were only recently acquired in laboratory settings [@verhulst_determinants_2006]. Although the affective component of food attitudes has been successfully investigated using the APP, previous studies have yielded mixed evidence for its utility in identifying the influence of cognitive components, such as health-related values, on implicitly measured food attitudes. While some studies have found that healthiness, or fat content, may have no influence on the affective priming effect [@becker_approach_2015; @roefs_at_2005], other evidence suggests that priming can reflect preference for low fat over high fat palatable foods, potentially attributed to health concerns [@roefs_early_2005].    
Overall, there has been moderate evidence to suggest that the APP can tap into the affective components of foods. Here we address three questions that are central to establishing the methodological utility of the APP in eating behaviour. First can priming effects be obtained for most liked and least liked foods, as expected by previous findings? Second, is this paradigm sensitive to cognitive components of attitudes, such as the healthiness of the foods? Finally, are priming effects for foods that vary in liking and healthiness associated with impulsive choices to consume these foods?

# Hypotheses {#hypotheses}

The proposed study tested several confirmatory hypotheses regarding the utility of the APP as an implicit measure of food attitudes. Priming effects were examined using both median reaction times for correct responses (RTs) and error rates (ERs). The relationship between priming measures and impulsive food choices were also be investigated. A schematic diagram of the APP contrasts and selected hypotheses is shown in Figure 1. In the \textit{\nameref{statistical_tests}} section, we tested four categories of predictions (H1-H4). These were the exclusive set of a priori hypotheses. For confirmatory analyses, all hypotheses were tested and reported with no changes to the specified IVs, DVs or any other variables, variable derivations, stated statistical transformations, or data exclusions within each test. The hypotheses, analyses, manipulated and non-manipulated variables, and measurements in \textit{\nameref{methods}} and \textit{\nameref{analyses}} sections were therefore complete, necessary, immutable and exclusive for all pre-registered confirmatory outcomes.

\begin{itemize}[noitemsep]
\item[H1.] Positive priming effect for non-food primes as a manipulation check for the APP\footnote{ The robustness of the manipulation check has been demonstrated in a series of four unregistered pilot experiments (see Online Appendix at https://osf.io/m92dv/).}\smallskip
\item[H1a.] RTs would be on average faster in congruent than incongruent non-food prime trials. 
\item[H1b.] ERs would be on average lower in congruent than incongruent non-food prime trials.
\end{itemize}


\begin{itemize}[noitemsep]
\item[H2.] Priming effects (RTs) for healthy and unhealthy foods\smallskip
\item[H2a.] RTs would be on average faster in congruent than incongruent food prime trials.
\item[H2b.] RTs would be on average faster in congruent than incongruent \textit{healthy} food prime trials, specifically.
\item[H2c.] RTs would be on average faster in congruent than incongruent \textit{unhealthy} food prime trials, specifically.
\item[H2d.] The priming effect (RT difference scores) would be on average greater for unhealthy than healthy most liked food prime trials (see \textit{\nameref{measures_indices}} for priming effect calculation).
\end{itemize}

\begin{itemize}[noitemsep]
\item[H3.] Priming effects (ERs) for healthy and unhealthy foods\smallskip
\item[H3a.] ERs would be on average lower in congruent than incongruent food prime trials.
\item[H3b.] ERs would be on average lower in congruent than incongruent \textit{healthy} food prime trials, specifically.
\item[H3c.] ERs would be on average lower in congruent than incongruent \textit{unhealthy} food prime trials, specifically.
\item[H3d.] The priming effect (ER difference scores) would be on average greater for unhealthy than healthy most liked food prime trials.
\end{itemize}

\begin{itemize}[noitemsep]
\item[H4.] Relationship between food choices and priming effects (RTs)\smallskip
\item[H4a.] The probability of choosing a most liked over a least liked food from within a pair of healthy food stimuli would positively correlate with the priming effect (RTs) in healthy food prime trials.
\item[H4b.] The probability of choosing a most liked over a least liked food from within a pair of unhealthy food stimuli would positively correlate with the priming effect (RTs) in unhealthy food prime trials.
\item[H4c.] The probability of choosing an unhealthy over a healthy most liked food would positively correlate with the difference in priming effects (RTs) between unhealthy and healthy most liked food prime trials.
\end{itemize}

\begin{figure} [!htb]
\centering
\includegraphics[width=\linewidth]{figures/Figure_1.png}
\caption{\textbf{Schematic diagram of affective priming paradigm contrasts and selected hypotheses.} \textbf{A.} Congruence in the affective priming paradigm is defined by the prime-target pairs. The trial is classified as \textit{congruent} when a most liked food prime is paired with a positive target, and \textit{incongruent} when paired with a negative target. Additionally, the trial is \textit{congruent} when a least liked food prime is paired with a negative target, and \textit{incongruent} when paired with a positive target. A priming effect for all foods (H2a) would be shown by faster sample means of median RTs (ms) in congruent vs. incongruent trials (RT\textsubscript{con} < RT\textsubscript{inc}). Details for all RT calculations can be found in \textit{\nameref{measures_indices}}. Priming effects for RTs (H2) are shown here only for demonstration purposes, but the priming effects in terms of ERs are in the same direction (ER\textsubscript{con} < ER\textsubscript{inc}; see H3 predictions). \textbf{B.} Priming effects are expected for both healthy (H2b) and unhealthy food primes (H2c). It is also hypothesised that the priming effect would be greater for unhealthy than healthy food primes (H2d). The RT priming effect was calculated as the difference in median RTs for incongruent and congruent trials (medianRT\textsubscript{inc} $-$ medianRT\textsubscript{con}) at the participant level and the sample means of these difference scores were then compared across conditions (healthy vs unhealthy).}
\end{figure}
\clearpage
\begin{figure}
    \ContinuedFloat
    \captionsetup{labelformat=empty}
    \caption{\textbf{C.} The probability of choosing unhealthy over healthy most liked foods in the unhealthy vs healthy food choice task trials was hypothesised to positively correlate (linearly) with individual differences in RT priming effects between unhealthy ($\Delta$RT\textsubscript{unhealthy}) and healthy ($\Delta$RT\textsubscript{healthy}) most liked food prime trials (H4c). The latter was examined using difference scores ($\Delta$RT\textsubscript{unhealthy} $-$ $\Delta$RT\textsubscript{healthy}) in which a positive value indicates participants had a larger priming effect for unhealthy most liked food primes. H4a and H4b are not shown here, but also posit expected positive linear correlations between variables. \textit{Note}. Hypotheses graphs are not based on actual or simulated data and are for illustrative purposes only. RT: reaction time; RT\textsubscript{con}: RTs in congruent trials; RT\textsubscript{inc}: RTs in incongruent trials; $\Delta$RT: RT difference score as shown in the formulas)}
\end{figure}
Pre-registered hypotheses for priming effects were proposed for both speed (RT) and accuracy (ER) measures. In response priming procedures without strict time windows (e.g., 300-450ms) priming effects are most commonly observed in RTs [@wentura_practical_2010], but we assume that such effects may be observed in either speed and/or accuracy performance (RT\textsubscript{con} < RT\textsubscript{inc} and/or ER\textsubscript{con} < ER\textsubscript{inc}). In addition, accuracy data should be inspected for potential speed-accuracy trade-offs (SATs). For example, participants could purposefully delay their responses on incongruent trials to improve accuracy, producing a priming effect for RTs but a reverse effect for error rates (i.e., ER\textsubscript{con} > ER\textsubscript{inc}). Therefore, support for observed priming effects would be dependent on both speed and accuracy hypotheses, as shown in the expression\footnote{Logical operators: $\neg$ = `not'; $\lor$ = `or'; $\land$ = `and'} below, where there should be no effects in the opposite direction (RT\textsubscript{con} > RT\textsubscript{inc} or ER\textsubscript{con} > ER\textsubscript{inc}) and there should be evidence for either RT or ER priming effects (RT\textsubscript{con} < RT\textsubscript{inc} or ER\textsubscript{con} < ER\textsubscript{inc}). 

{$\neg$ [(RT\textsubscript{con} > RT\textsubscript{inc}) $\lor$ (ER\textsubscript{con} > ER\textsubscript{inc})]} $\land$ [(RT\textsubscript{con} < RT\textsubscript{inc}) $\lor$ (ER\textsubscript{con} < ER\textsubscript{inc})]

A contingent analysis plan for testing these hypotheses (i.e., follow-up tests) when the effects were not in the expected direction was pre-registered (see \textit{\nameref{statistical_tests}}).

# Methods {#methods}

## Data collection protocol {#data_collection}

### Participants {#participants}

Recruitment was conducted via advertisements at Cardiff University and Prolific\footnote{Prolific requires pre-screening of participants and we set the current country of residence to UK for two reasons: 1) consistency of subject pools between laboratory and online settings; and 2) food brands included in the behavioural tasks may not be popular outside the UK.} (https://www.prolific.ac/) and data were collected in both online and laboratory settings (see \textit{\nameref{study_setting}}). 206 individuals were recruited via Prolific and we recorded 37 drop-outs in total.  For laboratory data collection we recruited 81 participants. 43 participants recruited via the Experimental Management System (EMS) received course credits when eligible (e.g., undergraduate students) and 36 participants not eligible for course credits received monetary reimbursement (£6). Participants performing the study via Prolific were rewarded £4.50 upon completion\footnote{We have used the £6.00/h rate for both Prolific and EMS participants. On Prolific, the estimated time of completion was 45 minutes, and, in the laboratory, we expected the study to last 60 minutes due to the coordination of group testing settings and the time it would take to provide all participants with the chosen food items (see \textit{\nameref{procedure}}).}.  
The complete and exhaustive set of inclusion and exclusion criteria for participation in the study were as follows. Eligible participants were at least 18 years of age, had normal or corrected-to-normal vision, including normal colour vision, and spoke English as their first or second language. Exclusion criteria included being on a diet and/or have recently been taking diet pills, a past and/or current history of eating disorders and food allergies and/or intolerances. Screening survey questions can be found in Supplementary Material 3 and all criteria were based on self-report. Further post-hoc exclusions of participants from pre-registered analyses are explained in \textit{\nameref{data_exclusions}} and presented in \nameref{characteristics}.  

The study has been approved by the local Research Ethics Committee at the School of Psychology, Cardiff University. All eligible participants provided informed consent and were debriefed. The study employed a within-subjects design and blinding of participants and/or experimenters was not applicable. However, participants were not made aware of the study aims before completion. Also, the contact between the experimenter and participants was minimised as data was collected online and in group laboratory settings, as explained in the next section.

### Study setting {#study_setting}

The study was undertaken in both laboratory (group testing) and online settings using Inquisit 5 [@inquisit_5]. The study protocol was matched for the two collected data sets, which were analysed and reported separately. The primary dataset stemmed from the laboratory setting, as this would allow us to examine consequential food choices (see \textit{\nameref{food_choice}}). The online dataset may directly replicate any findings on the APP as an implicit measure of food attitudes (H1-H3) and examine whether priming measures were associated with non-consequential food choices (i.e., choices are not motivated by the offer of real food at the end of the experiment). This data collection protocol would also provide insights into data quality and potential differences in the utility of the APP between laboratory and online settings.

### Sampling plan {#sampling}

A Sequential Bayes Factor (SBF) design with maximal N [@schonbrodt_sequential_2017] was employed, whereby data collection continued until either the desired level of evidence for all confirmatory hypotheses is obtained within each study setting separately (online and laboratory), or the maximal N had been reached. We set a minimum sample size per study setting, or nmin, of 40 and the maximum number of participants, or nmax, was 200. A threshold of BF\textsubscript{10} $\geq$ 10 would indicate strong evidence for the alternative hypothesis (H1) compared with the null (H0) while threshold of BF\textsubscript{01} $\geq$ 10 would correspond to strong evidence for H0 relative to H1 [see @lee_bayesian_2013]. Interim analyses were conducted for every 10 participants. The evidential value and hence interpretation of the results was exclusively based on Bayes factors, but frequentist statistics have also been reported ($\alpha$=.05). The analyses were only initiated when nmin had been reached and for frequentist statistics, analyses were to be conducted when data collection had been terminated. We declare that data collection procedures did not deviate from the proposed sampling plan. Although frequentist power analysis was not appropriate for an SBF design, we conducted a Bayes Factor Design Analysis (BFDA; see Figure B1) to assess the probability of the proposed design generating misleading evidence [@schonbrodt_bayes_2017]. Analyses were performed for all pre-registered hypotheses, as in directional t-tests for priming-related hypotheses (H1-H3) and directional correlations for food choice task predictions (H4), as shown in panels A and B of figure B1, respectively. The design priors were consistent with the analysis priors that would be employed for Bayesian t-tests and correlations (see \textit{\nameref{statistical_tests}}). Only the BFDA results were considered for the design of the study and no other power analyses were conducted. 

## Procedure {#procedure}

Recruited participants confirmed their eligibility and proceeded to provide their consent and choose their study setting (laboratory or online). Participants also indicated their dominant, or preferred, hand for performing the study tasks. A schematic of the study procedure is shown in Figure 2. The prime selection process required participants to complete a rating task where they rated how much they like food and non-food stimuli (see \textit{\nameref{primes}}). Participants completed a short APP practice block (16 trials), where they received feedback on both the speed and accuracy of their responses. Participants completed eight blocks of the task in total, with short breaks in between and instruction reminders.  

After the APP, participants performed a food choice task (FCT; see \textit{\nameref{food_choice}}), consisting of two blocks in total. In laboratory settings, participants received a food item chosen during the task at the end of the study. In online settings, food choice was not consequential in terms of real consumption. Ratings for all primes and targets (see \textit{\nameref{ratings}}) were provided after the FCT for exploratory analyses. Participants were presented with three short questionnaires\footnote{Questionnaire items may prime participants to pay attention to health- or weight-related information and therefore were presented after the behavioural tasks had been completed.} (see \textit{\nameref{questionnaires}}). The total duration of the study per participant was 40-50 minutes, after which participants were debriefed.

## Affective priming paradigm {#app}

### Prime selection {#primes}

The food primes were selected from 25 healthy and 25 unhealthy foods which were rated on liking, as measured using a visual analogue scale (VAS) ranging from -100 (“Strongly dislike”) to 100 (“Strongly like”). Four unhealthy and four healthy foods that had the maximum rating were selected as ‘most liked’ primes, and four unhealthy and healthy foods that had the minimum rating were chosen as ‘least liked’ primes. For each selected food category (e.g., apples for healthy- most liked), there were two exemplars in the APP. Instructions highlighted that “the rating task includes foods that could be either liked or disliked” to minimise the potential of social desirability bias whereby participants consistently rate foods on the positive end of the scale. Non-food primes were selected from 25 positive images from various categories such as animals, that comprised of several items (e.g., kitten, puppy, panda). The food ratings were always presented first and the order of healthy and unhealthy food rating blocks was randomised\footnote{All randomisation procedures mentioned hereinafter were implemented using Inquisit 5 functions.} across participants. Foods in each block were presented in a random order. More details about the food and non-food stimuli can be found in Supplementary Material 1 (e.g., nutritional characteristics). \newpage

\begin{figure} [H]
\centering
\includegraphics[width=\linewidth]{figures/Figure_2.png}
\captionof{figure}{\textbf{Schematic of the proposed study procedure and affective priming paradigm.} \textbf{A.} Primes for the affective priming paradigm (APP) were selected based on participants’ liking ratings, with the subsequent APP consisting of eight blocks, including 32 food and 8 non-food prime trials per block. The food choice task followed the APP and included two blocks of 64 trials. Participants then rated all primes and targets and were presented with three short questionnaires in the depicted order. \textbf{B.} The APP involved an evaluative categorisation task, where participants categorised target words as positive or negative as quickly and accurately as possible. After a central fixation cross (1000ms), a prime (food or non-food) was presented for 233ms, followed by a mask. Participants must respond within 1500ms of target onset using the “G” and “H” keys for positive/negative (randomised across participants) using their index and middle fingers. Finger placement on the assigned keys depended on the participant’s dominant hand.}
\end{figure}


### Task design {#design}

The APP involved an evaluative categorisation task (see Figure 2) in which participants categorised target words as either positive or negative. The targets were preceded by either ‘positive’ or ‘negative’ food primes, as well as positive non-food primes (manipulation check). The food prime trials involved a 2 $\times$ 2 $\times$ 2 design with the manipulated variables of healthiness (healthy versus unhealthy), affective congruence (congruent versus incongruent) and liking (most liked versus least liked). Non-food prime trials differed only in affective congruence. Each block of 40 trials consisted of 16 healthy and 16 unhealthy food prime trials as well as 8 non-food prime trials. Congruent and incongruent prime-target pairs appeared with equal probability for all trials. There were 32 positive and 32 negative targets in total (see Supplementary Material 2), which appeared randomly with equal probability across two consecutive blocks for food prime trials. Targets for non-food prime trials were presented randomly across eight blocks\footnote{Due to the separate randomisation of targets for food and non-food prime trials, certain targets may have appeared twice in a block.}.  
Participants were instructed to categorise the words as quickly and as accurately as possible. Participants responded using the “G” and “H” keys, as explained in Figure 2. Each trial commenced with a central fixation cross followed after 1000ms by the prime. Following a 233ms inter-stimulus interval (ISI), the prime was succeeded by a backward mask (17ms) to limit subjective awareness of the primes, constructed from a mosaic of various food stimuli with different colour compositions [@wentura_practical_2010]. The stimulus-onset asynchrony (SOA) between prime and target was 250ms. The response window begun on target onset (i.e., 1250ms) and participants had a maximum reaction time (maxRT) of 1500ms. Each trial ended either when a response was registered or when the maximum total trial duration was reached (2750ms). A trial was considered incorrect if the target categorisation was wrong or participants did not respond within 1500ms.  
All stimuli were presented centrally and pictures had their relative dimensions set to 40% of the vertical and horizontal width of the presentation window. The targets and fixation cross (+) were presented in black, bold Arial fonts. Words were presented in upper-case letters against a uniform grey background.

## Food choice task {#food_choice}

The FCT involved binary food choices, adapted from previous literature [@veling_training_2017; @zoltak_attention_2018]. Participants were instructed to choose the foods that they would prefer to eat at the end of the experiment. To measure consequential food choices, in laboratory settings, participants were instructed that one of their choices would be selected by the researcher(s) and they would be given the food item they had chosen on that occasion. The selection of the food was not random due to the unsuitability of certain foods for laboratory storage (e.g., fast decay of fruits). The proposed selection process was in line with instructions used in previous literature [@veling_training_2017].  The researcher(s) selected an item from the list of suitable foods and restricted selection to foods rated as ‘most liked’ by the participants (see Supplementary Material 1 for details). In online settings, participants did not receive a food item at the end of the study and thus choices were not consequential. In the laboratory, we also provided participants with bottled water after screening to minimise the potential impact of thirst levels on food choices.  
Each trial in the FCT (see Figure 3) involved the simultaneous presentation of two food items on the left and right of a central fixation cross, which participants would choose between using the “C” and “M” keys\footnote{We purposefully deviated from the previous literature on single-hand responses, due to the possibility of learned associations (e.g., between the index/middle finger commands for ‘positive’ and ‘negative’ in the APP and food choices). Here, participants responded using both hands, by placing their index fingers on the “C” and “M” keys.}. A response had to be registered within a maximum of 1500ms and participants would then be presented with response feedback (500ms) where their confirmed choice would be highlighted (i.e., a yellow frame around the selected food). A central fixation cross was presented during the intertrial interval (duration = 1000-2000ms\footnote{Random selection in steps of 100ms}).\newpage
 
\begin{figure} [H]
\centering
\includegraphics[width=0.8\linewidth]{figures/Figure_3.png}
\captionof{figure}{\textbf{Schematic of the food choice task and different trial types.} \textbf{A.} Participants made binary food choices within 1500ms between two food items presented on the left and right of a central fixation cross. Their response was followed by visual feedback (500ms). The intertrial interval (ITI) varied randomly from 1000-2000ms.\textbf{B.} The two main trial types involved unhealthy vs healthy and most liked vs least liked choices. There were two categories of food pairs for each trial type. In \textit{unhealthy vs healthy} trials (N=32 per block), participants chose between most liked foods (N=16) or least liked foods (N=16), as shown in the example matrix. In \textit{most liked vs least liked} trials, participants chose between unhealthy (N=16) or healthy foods (N=16). There were two blocks in total and choices were repeated in block 2 to counterbalance stimulus positions, as shown above.}
\end{figure} \newpage

Participants were instructed to make their choices quickly and time pressure would help ensure that food choices are not deliberate, reducing the probability of demand characteristics [@veling_training_2017]. Feedback was presented if participants do not respond within 1500ms, instructing them to choose faster (“Please try to choose faster” - 1000ms). To avoid loss of data, missed trials were be repeated and only one repetition per trial was allowed. For each design cell of the APP (healthiness $\times$ liking) there were four food categories included in the FCT. All food prime categories were included in the FCT and represented by the primary exemplars (i.e., stimuli used in prime selection). Two main types of trials were presented and each type had two categories (see panel B in Figure 3). The FCT comprised 128 binary choices in total and was split into two blocks of 64 trials with a short intervening break.

## Prime & target ratings {#ratings}

Participants explicitly evaluated all prime categories and targets for exploratory analyses. Food primes were evaluated for valence, arousal, perceived healthiness and frequency of cravings. Non-food primes were also evaluated for valence and arousal. Ratings were only obtained for the primary exemplars. All targets were evaluated for valence and arousal. Rating scales can be found in Supplementary Material 3.

## Questionnaires {#questionnaires}

### Trait & state variables {#trait_state}

An initial survey recorded measures for exploratory analyses (see Supplementary Material 3). The questionnaire measured several trait and state variables that could be associated with eating behaviours and related information. These variables included how long ago they had their last meal, whether they follow a specific diet and hunger levels. Self-reported height and weight was  recorded in order to calculate the participants body mass index (BMI: kg/m\textsuperscript{2}). Participants also indicated their gender and ethnicity (optional).

### Food Cravings Questionnaire- Trait Reduced {#fcq_tr}

Participants completed the short version of the Food Cravings Questionnaire – Trait Reduced [FCQ-T-r; @meule_short_2014] in order to examine trait food cravings as a potential moderator of affective priming effects. FCQ-T-r consists of 15 items scored on a 5-point scale (“strongly disagree” to “strongly agree”). The subscales included in the FCQ-T-r are ‘lack of control over eating’, ‘thoughts or preoccupation with food’, ‘intentions and plans to consume food’, ‘emotions before or during food craving’, and ‘cues that may trigger food craving’.

### Follow-up study questionnaire {#follow_up}

At the end of the study participants completed a follow-up study questionnaire (see Supplementary Material 3), where they were asked to answer questions about their performance in the APP (e.g., response strategies) and report which target words, if any, they considered ambivalent or unclear. Participants also indicated the number of occasions they were interrupted during the word task [see @waters_evaluating_2008]. The survey included an instructional manipulation check (IMC) which has been designed to examine whether participants are paying attention to the instructions as well as a questionnaire attention check measure [@kees_analysis_2017]. These measures are further explained in Supplementary Material 3. Participants performance on the data quality assurance measures would be later compared for online and laboratory settings in exploratory analyses.

# Analyses {#analyses}

## Pre-registered analyses {#prereg_analyses}

### Measures & indices {#measures_indices}

All planned comparisons are outlined in \textit{\nameref{statistical_tests}}, where RT\textsubscript{con} and RT\textsubscript{inc} denote the sample mean of individual median correct RTs in congruent and incongruent trials, and ER\textsubscript{con} and ER\textsubscript{inc} refer to the mean error rates in congruent and incongruent trials, respectively. At the level of participants, median RTs were used as they are less sensitive to outliers and may provide a more accurate measure of central tendency in positively-skewed distributions. The median RTs were computed for each participant and then a Bayesian paired-samples t-test was conducted for the alternative hypothesis that the population mean of the difference in median RTs is smaller than 0. The difference in median RTs for each participant between congruent and incongruent trials (medianRT\textsubscript{inc} $-$ medianRT\textsubscript{con}) was then calculated for further testing of RT priming effects. The sample means of these difference scores were then compared across conditions (e.g., $\Delta$RT\textsubscript{healthy} < $\Delta$RT\textsubscript{unhealthy} in H2d) and are referred to as $\Delta$RT. Similarly, $\Delta$ER was defined as the priming effect for error rates, where $\Delta$ER = ER\textsubscript{inc} $-$ ER\textsubscript{con}. For the calculation of error rates at the participant level, accuracy is recoded as 1=incorrect and 0=correct.  
With regards to FCT analyses, p(unhealthy|most liked) refers to the conditional probability of choosing an unhealthy food in the unhealthy vs healthy food choice trials when most liked food pairs are presented (see Figure 3 for trial types). Accordingly, p(most liked|healthy) denotes the conditional probability of choosing a most liked food in the most liked vs least liked trials where healthy food pairs are presented, and p(most liked|unhealthy) indicates the conditional probability of choosing a most liked food on trials where the unhealthy food pairs are presented. Choices were recoded according to trial types to compute these probabilities. For example, in trials where participants chose between most liked and least liked foods and the foods presented were healthy, choices were coded as 1=most liked and 0=least liked. Then the mean was calculated and denoted the probability that participants chose a most liked food in these most liked vs least liked (healthy) choice trials, that is p(most liked|healthy). Probability values were calculated from the number of completed trials. The difference in priming effects (RTs only) between unhealthy and healthy most liked food prime trials is represented by $\Delta$RT\textsubscript{unhealthy} $-$ $\Delta$RT\textsubscript{healthy}. All computations of pre-registered measures can be found in the openly shared analyses scripts available at https://osf.io/3hrmy/.

### Statistical tests {#statistical_tests}

Bayesian paired-samples t-tests [@rouder_ttest] employed a prior with the $\sqrt{2/2}$ scale parameter for the half-Cauchy distribution. Bayesian correlation pairs had a stretched beta with the parameter $\gamma$=1, which corresponds to a uniform prior [@wagenmakers_how_2016]. Every directional Bayesian test would be reported with the equivalent frequentist test (see 
\textit{\nameref{sampling}}). Analyses were conducted separately for the online and laboratory datasets and results were reported independently (see \textit{\nameref{study_setting}}). More information on software, analyses scripts and statistical transformations can be found in the Appendix.

H1, H2 and H3 were exclusively tested using directional Bayesian paired- samples t-tests, as outlined below.

\begin{itemize}[noitemsep]
\item[H1a.] RT\textsubscript{con} < RT\textsubscript{inc} for non-food prime trials
\item[H1b.] ER\textsubscript{con} < ER\textsubscript{inc} for non-food prime trials
\item[H2a.] RT\textsubscript{con} < RT\textsubscript{inc} for food prime trials
\item[H2b.] RT\textsubscript{con} < RT\textsubscript{inc} for healthy food prime trials
\item[H2c.] RT\textsubscript{con} < RT\textsubscript{inc} for unhealthy food prime trials
\item[H2d.] $\Delta$RT\textsubscript{healthy} < $\Delta$RT\textsubscript{unhealthy} for most liked food primes
\item[H3a.] ER\textsubscript{con} < ER\textsubscript{inc} for food prime trials
\item[H3b.] ER\textsubscript{con} < ER\textsubscript{inc} for healthy food prime trials
\item[H3c.] ER\textsubscript{con} < ER\textsubscript{inc} for unhealthy food prime trials
\item[H3d.] $\Delta$ER\textsubscript{healthy} < $\Delta$ER\textsubscript{unhealthy} for most liked food primes
\end{itemize}

H4 was only examined via directional Bayesian correlation pairs, as shown below. The reported correlation coefficient was Pearson’s rho.

\begin{itemize}[noitemsep]
\item[H4a.] $\Delta$RT\textsubscript{healthy-most liked} positively correlates with p(most liked|healthy) in most liked vs least liked food choice trials with healthy food pairs.
\item[H4b.] $\Delta$RT\textsubscript{unhealthy-most liked} positively correlates with p(most liked|unhealthy) in most liked vs least liked food choice trials with healthy food pairs.
\item[H4c.] $\Delta$RT\textsubscript{unhealthy-most liked} $-$ $\Delta$RT\textsubscript{healthy-most liked} positively correlates with p(unhealthy|most liked) in unhealthy vs healthy food choice trials
\end{itemize}

As a contingent analysis plan, we may also report BFs for hypotheses H1, H2 and H3 in the opposite direction. If differences between means are descriptively in the unexpected (positive) direction, such as RT\textsubscript{con} > RT\textsubscript{inc} for food prime trials, BFs for the opposite direction would be presented in addition to pre-registered test values. The decision to report the positive one-sided tests would be based on descriptive values and not on BFs, as support for the null in a directional Bayesian t-test does not exclude the possibility that there is greater evidence for an effect in the opposite direction. For example, even if there is adequate evidence for H0 and the null hypothesis is preferred to the negative hypothesis (RT\textsubscript{con} < RT\textsubscript{inc}), the positive hypothesis (RT\textsubscript{con} > RT\textsubscript{inc}) may still be favoured over the null [see @morey_bayesfactor_2014-1].   
Although statistical tests were not conducted for performance in the FCT alone, such as reaction time differences between trial types, we would report descriptively the probabilities of choosing least liked food items in the most liked vs least liked trials when both healthy and unhealthy food pairs are presented, which should be below 0.5 if prime selection according to liking was successful [see also @veling_training_2017].  
For all t-tests (H1-H3), we would report Cohen’s \textit{d\textsubscript{av}} which uses the average standard deviation of both measures in a paired-samples comparison (see Appendix A for formula) and can be similar to Cohen’s \textit{d\textsubscript{s}} effect size for between-subject designs, increasing its utility for potential meta-analyses [@lakens_calculating_2013-1]. 

### Data exclusions {#data_exclusions}

Error rates in the APP were inspected for food and non-food prime trials separately and participants with ERs greater or equal to 0.4 from within either set of trials were excluded from all respective analyses\footnote{Please note that deviations from the pre-registered protocol with respect to confirmatory analyses and data exclusions would not be possible with the provided R scripts.}. This obviated the need for further inspection of the distribution of missed or inaccurate responses across conditions. The FCT data were inspected for missed responses, where participants did not respond within 1500ms. Analyses conducted for H4a, H4b, and H4c would not include participants who had more than 50% of missed trials across the two blocks in any trial type examined under H4 (i.e., < 16 out of 32 trials).  
Data were also inspected for timing delays in trial events in the APP due to the possible occurrences of technical issues during online testing (e.g., slow broadband). Timing delays were defined as trial events that last two or more screen refreshes than originally programmed. The trial events that were inspected were the presentation of the prime (233ms) and mask (17ms) and trials with timing delays would be removed from analyses. If a participant had more than 25% of trials removed, they would then be excluded from all analyses.

# Results

```{r load_samples, echo=FALSE}
sample1 <- read.csv("sample1_chars.csv")
sample2 <- read.csv("sample2_chars.csv")
```

##Sample characteristics {#characteristics}

Collected data were inspected for potential exclusions, as outlined above (see \textit{\nameref{data_exclusions}}). No participants were excluded due to timing delays in the APP from the laboratory sample. We removed delayed trials from 7 participants (only 1-3 trials per participant) who completed the study online. Two participants were excluded based on their total proportion of error rates in the APP (>=0.4) from the laboratory sample and three participants were removed from the online data. The final samples for APP analyses (H1-H3) were 79 (\textit{M\textsubscript{age}} = `r mean(sample1$Age)`, \textit{SD\textsubscript{age}} = `r sd(sample1$Age)`) and 166 (\textit{M\textsubscript{age}} = `r mean(sample2$Age)`, \textit{SD\textsubscript{age}} = `r sd(sample2$Age)`) for laboratory and online settings, respectively. For FCT analyses (H4), we also excluded one participant from each sample because they had missed more than 25\% of FCT trials. 

```{r}
SDhunger1 = sd(sample1$Hunger)
SDhunger2 = sd(sample2$Hunger)
```

All recorded sample characteristics will be found in Supplementary Material 4. Here we present variables that may be methodologically crucial for the study of eating behaviours and related cognitive/affective processes. In the laboratory cohort, the mean self-reported hunger level of the participants at the time of the study was `r mean(sample1$Hunger)` (\textit{SD} = `r sd(sample1$Hunger)`) and 54.43\% of individuals had their last meal 1-3 hours before the study, while 20.25\% had eaten less than 1 hour before. 82.28\% of all laboratory participants reported that they were not following any specific diet. 65 participants provided valid height and weight measurements and the calculated BMI ranged between 17.91 and 30.66 kg/m\textsuperscript{2} (\textit{M\textsubscript{BMI}} = `r mean(sample1$BMI, na.rm=TRUE)`, \textit{SD\textsubscript{BMI}} = `r sd(sample1$BMI, na.rm=TRUE)`). In the online cohort, three individuals did not complete the questionnaires. On average, participants had a hunger level of `r mean(sample2$Hunger)` (\textit{SD} = `r sd(sample2$Hunger)`) and 54.88\% had a meal 1-3 hours before the study, consistent with the laboratory cohort. 123 out of 165 participants provided valid measurements for the calculated BMI, which ranged between 17.63 and 34.75 kg/m\textsuperscript{2} (\textit{M\textsubscript{BMI}} = `r mean(sample2$BMI, na.rm=TRUE)`, \textit{SD\textsubscript{BMI}} = `r sd(sample2$BMI, na.rm=TRUE)`).

```{r load_app_data, echo=FALSE}
food1_app <- read.csv("food1_app.csv")
food2_app <- read.csv("food2_app.csv")

nonfood1_app <- read.csv("nonfood1_app.csv")
nonfood2_app <- read.csv("nonfood2_app.csv")
```

```{r table_mods, echo=FALSE}
table1 <- read.csv("table1.csv")
table2 <- read.csv("table2.csv")

specify_decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))

table1$p <- specify_decimal(table1$p, 3)
table1$p = as.character(table1$p)
table1$p = ifelse(table1$p == "0.000", "<.001", paste("  ", sep=" ", substr(table1$p, 2, 5)))

table2$p <- specify_decimal(table2$p, 3)
table2$p = as.character(table2$p)
table2$p = ifelse(table2$p == "0.000", "<.001", paste("  ", sep=" ", substr(table2$p, 2, 5)))

#Optional - now I am splitting H1 from table to save space 
table1F <- table1[3:10, ]
table2F <- table2[3:10, ]

#When dimnames (aka rownames) change on the dataframe the apa_table doesn't knit and returns an error
row.names(table1F) <-1:nrow(table1F)
row.names(table2F) <-1:nrow(table2F)

table_nf <- rbind(table1[1:2, ], table2[1:2, ])

table_nf$Hypothesis <- as.character(table_nf$Hypothesis)
table_nf$Hypothesis[1] <- "H1a (Laboratory)"
table_nf$Hypothesis[2] <- "H1b (Laboratory)"
table_nf$Hypothesis[3] <- "H1a (Online)"
table_nf$Hypothesis[4] <- "H1b (Online)"

#table_nf$t <- round(table_nf$t, digits=2)

#table_nf$t[1] <- paste(table_nf$t[1], "*")
#table_nf$t[3] <- paste(table_nf$t[3], "**")
```

##Manipulation check {#manipulation_check}

We obtained *extreme* evidence for the expected RT priming effects on non-food prime trials (H1a), as presented in Table\ \@ref(tab:nonfoods). In the laboratory cohort, participants were faster to respond in congruent (\textit{$\bar{M}$}\footnote{From hereonafter, we use the abbreviation \textit{$\bar{M}$} to refer to the sample mean of median RTs calculated at the participant level, as described in \textit{\nameref{measures_indices}}.} = `r mean(nonfood1_app$con_RT)`; \textit{SD} = `r sd(nonfood1_app$con_RT)`) compared to incongruent non-food trials (\textit{$\bar{M}$} = `r mean(nonfood1_app$inc_RT)`; \textit{SD} = `r sd(nonfood1_app$inc_RT)`). This effect was replicated in the online cohort, with participants having, on average, lower median RTs on congruent (\textit{$\bar{M}$} = `r mean(nonfood2_app$con_RT)`; \textit{SD} = `r sd(nonfood2_app$con_RT)`) rather than incongruent non-food trials (\textit{$\bar{M}$} = `r mean(nonfood2_app$inc_RT)`; \textit{SD} = `r sd(nonfood2_app$inc_RT)`). RT priming effects from non-food prime trials can be seen in Figure 4.   
In support of H1b, we observed an overall priming effect for error rates in the expected direction. In the laboratory cohort, there was *strong* evidence that participants had lower error rates on congruent (\textit{$\bar{M}$} = `r mean(nonfood1_app$con_ER)`; \textit{SD} = `r sd(nonfood1_app$con_ER)`) compared to incongruent non-food trials (\textit{$\bar{M}$} = `r mean(nonfood1_app$inc_ER)`; \textit{SD} = `r sd(nonfood1_app$inc_ER)`). Similarly, in the online cohort we obtained *very strong* evidence for error rates being reduced from congruent (\textit{$\bar{M}$} = `r mean(nonfood2_app$con_ER)`; \textit{SD} = `r sd(nonfood2_app$con_ER)`) to incongruent non-food trials (\textit{$\bar{M}$} = `r mean(nonfood2_app$inc_ER)`; \textit{SD} = `r sd(nonfood2_app$inc_ER)`). The support for the pre-registered hypotheses which were introduced as a manipulation check for the APP is paramount to the validity of the employed task design for food stimuli.

<!--
```{r nonfoods, results="asis", message=FALSE, fig.pos='H'}
apa_table(table_nf, 
          font_size = "small", 
          align = c('l','r','c','c','c','c','c','c','c'),
          placement = "htb", 
          caption = "Statistical test results for the manipulation check (H1) from both laboratory and online cohorts ",
          col.names=c(" ", "\\textit{BF}\\textsubscript{10}", "\\textit{t}*", "\\textit{p}", "\\textit{MDiff}", 
                      "\\textit{d\\textsubscript{av}}", "Lower", "Upper", "Conclusion"), 
          col_spanners = list(`95$\\%$ CI for \\textit{d\\textsubscript{av}}` = c(7, 8)),
          note = "* Laboratory cohort: df = 78; Online cohort: df = 166")
```
-->
\begin{table}[H]
\begin{center}
\begin{threeparttable}
\caption{\label{tab:nonfoods}Statistical test results for the manipulation check (H1) from both laboratory and online cohorts }
\small{
\begin{tabular}{lrccccccc}
\toprule
 &  &  &  &  &  & \multicolumn{2}{c}{95$\%$ CI for \textit{d\textsubscript{av}}}  &\\
\cmidrule(r){7-8}
  & \textit{BF}\textsubscript{10} & \textit{t}* & \textit{p} & \textit{MDiff} & \textit{d\textsubscript{av}} & Lower & Upper & Conclusion\\
\midrule
H1a (Laboratory) & 2,888.12 & -4.68 & <.001 & -20.54 & -0.32 & -0.46 & -0.18 & Supported\\
H1b (Laboratory) & 18.68 & -3.08 & \ \  .001 & -0.03 & -0.37 & -0.61 & -0.13 & Supported\\
H1a (Online) & 1,204,897,684.36 & -7.28 & <.001 & -26.28 & -0.36 & -0.46 & -0.25 & Supported\\
H1b (Online) & 54.02 & -3.48 & <.001 & -0.03 & -0.36 & -0.56 & -0.15 & Supported\\
\bottomrule
\addlinespace
\end{tabular}
}
\begin{tablenotes}[para]
\normalsize{\textit{Note.} * Laboratory cohort: df = 78; Online cohort: df = 166}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table} \newpage


\begin{figure} [H]
\centering
\includegraphics[width=\linewidth]{figures/nonfood_primes.png}
\captionof{figure}{\textbf{Plots showing the observed reaction time priming effects for non-food primes in the laboratory and online cohorts.} The boxplots (A, B) show the individual median reaction times (RTs) calculated for non-food prime trials at the participant level and how they differ for each level of affective congruence. As expected, participants were faster to respond on congruent compared to incongruent trials in both the laboratory (panel \textbf{A}) and online cohorts (panel \textbf{B}). The RT priming effect, or $\Delta$RT, was calculated as the difference between incongruent and congruent RTs (RT\textsubscript{inc} $-$ RT\textsubscript{con}) and can be seen on the presented scatterplots. Any value shown above the abline on these plots corresponds to a positive priming effect, whereby incongruent RTs (y-axis) are larger compared to congruent RTs (x-axis). The magnitude of $\Delta$RT (ms) can be visualised using the colour gradient applied to plotted data points.}
\end{figure} \newpage

##Food priming effects & choice behaviour {#food_priming_effects}
###Findings from primary laboratory cohort
```{r load_fct_data, echo=FALSE}
FCT1 <- read.csv("FCT1.csv")
FCT2 <- read.csv("FCT2.csv")
```
The results of all statistical tests for pre-registered hypotheses H2 and H3 from the laboratory cohort (N=79) are presented in Table\ \@ref(tab:foods1). We obtained *extreme* evidence for an RT priming effect across food prime trials (H2a- see Figure 5A) and healthy food prime trials (H2b) as well as *very strong* evidence for an effect across unhealthy food prime trials (H2c). Although we expected that the RT priming effect would be reduced from healthy compared to unhealthy most liked food prime trials, we found *anecdotal* evidence for the alternative hypothesis compared to the null. As explained in the \textit{\nameref{hypotheses}} section, we pre-registered that support for any observed priming effects would be dependent on both speed- and accuracy-related hypotheses. Priming effects for error rates were in the expected direction across food prime trials and therefore we can conclude that any RT effects were not observed due to strategic responding or SATs, such as trading off high accuracy for faster responses. There was *extreme* evidence for ER priming effects for both healthy and unhealthy food prime trials (H3a, H3b, H3c). We found *anecdotal* evidence for the null compared to the alternative hypothesis with regards to differences in ER priming effects between healthy and unhealthy food primes (H3d; \textit{BF}\textsubscript{01} = 1.47).  
Bayesian correlation pairs for hypotheses H4a, H4b have yielded *moderate* evidence regarding the absence or presence of the expected linear positive correlations. The probability of choosing a most liked food over a least like food from within a pair of healthy food stimuli (\textit{M} = `r mean(FCT1$p.mh)`, \textit{SD} = `r sd(FCT1$p.mh)`) did not positively correlate with the RT priming effect in healthy food prime trials [H4a; \textit{BF}\textsubscript{01} = 7.80, $\rho$ = -0.02, \textit{p} = .551]. Similarly, the probability of choosing a most liked over a least liked food from within a pair of unhealthy food stimuli (\textit{M} = `r mean(FCT1$p.mu)`, \textit{SD} = `r sd(FCT1$p.mu)`) did not positively correlate with the RT priming effect in unhealthy food prime trials [H4b; \textit{BF}\textsubscript{01} = 9.78, $\rho$ = -0.05, \textit{p} = .676]. We obtained strong evidence for the null compared to the alternative hypothesis for H4c. The probability of choosing an unhealthy over a healthy most liked food (\textit{M} = `r mean(FCT1$p.um)`, \textit{SD} = `r sd(FCT1$p.um)`) did not positively correlate with the difference in RT priming effects between unhealthy and healthy most liked food prime trials [H4c; \textit{BF}\textsubscript{01} = 13.64, $\rho$ = -0.12, \textit{p} = .850].

```{r foods1, results="asis", message=FALSE}
#For now degrees of freedom are added manually to the column for the t statistic, e.g. t(78)
apa_table(table1F, 
          font_size = "small", 
          align = c('l','r','c','c','c','c','c','c','c'),
          placement = "htb", 
          caption = "Statistical test results for hypotheses H2 and H3 from the laboratory cohort (N=79)",
          col.names=c(" ", "\\textit{BF}\\textsubscript{10}", "\\textit{t}(78)", "\\textit{p}", "\\textit{MDiff}", 
                      "\\textit{d\\textsubscript{av}}", "Lower", "Upper", "Conclusion"), 
          col_spanners = list(`95$\\%$ CI for \\textit{d\\textsubscript{av}}` = c(7, 8)))
```

###Replication: findings from online cohort

The results of all statistical tests for pre-registered hypotheses H2 and H3 from the online cohort (N=167) are presented in Table\ \@ref(tab:foods2). Reaction time and error rate priming effects were replicated in the online cohort. We obtained *extreme* evidence for an RT priming effect across food prime trials (H2a- see Figure 5B), as well as healthy and unhealthy food prime trials (H2b, H2c). Although in the laboratory cohort, there was only *anecdotal* evidence that the RT priming effect for healthy foods was not lower compared to the RT priming effect for unhealthy foods, in the online cohort we obtained *moderate* evidence for the absence of this expected difference (\textit{BF}\textsubscript{01} = 7.94). In line with the findings from the laboratory cohort, differences in error rates between congruent and incongruent food prime trials were in the expected direction. We found *extreme* evidence for ER priming effects across food prime trials (H3a) and healthy food prime trials specifically (H3b). There was also *very strong* evidence that error rates were lower on congruent compared to incongruent unhealthy food prime trials (H3c). In regards to the expected difference between ER priming effects for healthy and unhealthy food primes, we report *moderate* evidence for the null compared to the alternative hypothesis (\textit{BF}\textsubscript{01} = 9.30).
```{r foods2, results="asis", message=FALSE}
apa_table(table2F, 
          font_size = "small", align = c('l','r','c','c','c','c','c','c','c'), 
          placement = "htb",
          caption = "Statistical test results for hypotheses H2 and H3 from the online cohort (N=167)",
          col.names=c(" ", "\\textit{BF}\\textsubscript{10}", "\\textit{t}(166)", "\\textit{p}", "\\textit{MDiff}", 
                      "\\textit{d\\textsubscript{av}}", "Lower", "Upper", "Conclusion"), 
          col_spanners = list(`95$\\%$ CI for \\textit{d\\textsubscript{av}}` = c(7, 8)))
```
In the online cohort, we only found *anecdotal* evidence for the lack of a positive correlation between the probability of choosing a most liked food over a least like food from within a pair of healthy food stimuli (\textit{M} = `r mean(FCT2$p.mh)`, \textit{SD} = `r sd(FCT2$p.mh)`) and the RT priming effect in healthy food prime trials [H4a; \textit{BF}\textsubscript{01} = 3.05, $\rho$ = 0.09, \textit{p} = .125]. There was also *moderate* evidence for the absence of a positive linear correlation between the probability of choosing a most liked over a least liked food from within a pair of unhealthy food stimuli (\textit{M} = `r mean(FCT2$p.mu)`, \textit{SD} = `r sd(FCT2$p.mu)`) and the RT priming effect in unhealthy food prime trials [H4b; \textit{BF}\textsubscript{01} = 8.45, $\rho$ = 0.02, \textit{p} = .408]. In contrast to the laboratory cohort, where we obtained *strong evidence* for the null compared to the alternative, for H4c, in the online cohort there was only *anecdotal* evidence for the lack of a positive linear correation between the probability of choosing an unhealthy over a healthy most liked food (\textit{M} = `r mean(FCT2$p.um)`, \textit{SD} = `r sd(FCT2$p.um)`) and the difference in RT priming effects between unhealthy and healthy most liked food prime trials [H4c; \textit{BF}\textsubscript{01} = 3.29, $\rho$ = 0.09, \textit{p} = .137].

\begin{figure} [H] \label{foodprimes}
\centering
\includegraphics[width=\linewidth]{figures/food_primes.png}
\captionof{figure}{\textbf{Plots showing the observed reaction time priming effects for non-food primes in the laboratory and online cohorts.} Boxplots and scatterplots presented here demonstrate the differences between individual median reaction times on congruent and incongruent food prime trials, as previously explained (see Figure 4).}
\end{figure} \newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

<!--# Discussion-->
\appendix
\section{Pre-registered analyses software and script information}

Pre-processing of the data and analyses were exclusively conducted in RStudio [@rstudio] and all scripts are available at https://osf.io/3hrmy/. Scripts specify all required packages and relevant compatibility information (e.g., RStudio version). For Bayesian t-tests and correlations we used the ‘BayesFactor’ package [@bayesfactor] and for the reported frequentist tests the ‘jmv’ package [@jmv] was employed. The BFDA R package  [@schonbrodt_bfda_2018] was used for all analyses presented in Appendix B.   
The Shapiro-Wilk test of normality would be conducted for every t-test under H1, H2 and H3. If the normality assumption was violated under alpha < 0.005 for any of planned comparisons in a set of predictions for RTs (H1a, H2a-H2d), we would only report the results with log-transformed values (e.g., logRT\textsubscript{con} < logRT\textsubscript{inc} for H1a) as part of our Pre-registered analyses section. For example, if normality was violated for H2c under H2 (i.e., only RT difference scores for unhealthy food prime trials), we would log-transform RTs for all comparisons (H2a, H2b, H2c, H2d). For error-related predictions (H1b, H3a-H3d), Bayesian t-tests would be performed as planned. However, as part of the supplementary frequentist statistics that would be reported, Wilcoxon signed rank tests would also be performed. As explained in the \textit{\nameref{sampling}} section, conclusions would only be drawn based on Bayesian tests. \newpage

For Cohen’s \textit{d\textsubscript{av}} calculations, we used the formula from @lakens_calculating_2013-1, as shown below. The mean difference of the two repeated measures (mean of the difference scores) is divided by the average standard deviation of both measures. The R code has been adapted from an existing script from @anvari_using_2019 , available at https://osf.io/cfd6e/. The script also provides code for the calculation of confidence intervals (CIs) for Cohen’s \textit{d\textsubscript{av}}, which were also reported. 
\begin{align*}
Cohen’s\: d_{av}= \frac{Mean\: difference}{\sqrt{\frac{SD_1^2\: +\: SD_2^2}{2}}}
\end{align*}

\section{Bayes factor design analysis for proposed sampling plan}

\setcounter{figure}{0}  

\begin{figure} [!htb]
\renewcommand{\thefigure}{B\arabic{figure}}
\centering
\includegraphics[width=\linewidth]{figures/Figure_B1.png}
\caption{\textbf{Bayes factor design analysis (BFDA) results for H0 and H1 at different sample and effect sizes in a simulated sequential design.} }
\end{figure}
\clearpage
\begin{figure}
\captionsetup{labelformat=empty}
\caption{The graphs show the percentage of simulated studies (10,000) terminating at a boundary (H1 and H0) when the threshold is set to BF01 $\geq$ 10 and BF10 $\geq$ 10. BFDA has been conducted for the sample sizes of 40 (nmin), 80, 120, 160 and 200 (nmax). The Cohen’s dz and correlation $\rho$ (rho) values reflect the potential of the true effect size being either zero (i.e., H0 is true), ‘small’ (dz=0.2; $\rho$=0.1), ‘medium’ (dz=0.5; $\rho$=0.3) or ‘large’ (dz=0.8; $\rho$=0.5). These benchmarks are used for demonstration purposes only. \textbf{A. This panel shows the BFDA results for the planned directional Bayesian paired-samples t-tests (H1-H3).} For H0, 77.04\% of all simulated studies correctly terminate at the H0 boundary when nmax has been reached. However, the probability of obtaining false positive evidence is low, with only 1.5\% of the studies incorrectly stopping at the H1 boundary. At n=40, the probability is very low, with only 0.36\% of studies incorrectly stopping at the H1 boundary. Assuming a small true effect size for H1 (dz=0.2), at nmax 57.7\% of simulated studies terminate at the correct H1 boundary and 5.9\% of studies stop at the H0 boundary (i.e., probability of obtaining false negative evidence). Assuming a medium true effect size (dz=0.5), at a sample size of 120, 99.7\% of all studies correctly terminate at the H1 boundary and no studies (0\%) stop at the H0 boundary. For a large true effect size (dz=0.8), 80 participants would be adequate to correctly support H1 with 100\% of simulated studies correctly reaching the H1 boundary. \textbf{B. This panel shows the BFDA results for the planned directional Bayesian correlations (H4).} Assuming the absence of a positive correlation (H0) at nmax, 71.19\% of all simulated studies correctly terminate at H0 and 1.92\% incorrectly stop at the H1 boundary. For a small true effect size ($\rho$=0.1), only 14.85\% of studies correctly terminate at H1 when nmax is reached and 25.54\% of studies stop at H0. For a medium true effect size ($\rho$=0.3), at nmax 95.65\% of all studies correctly provide strong evidence for H1 and for a large effect size ($\rho$=0.5) at the sample size 120, 99.94\% of all simulated studies correctly terminate at the H1 boundary. Note. The code and output of the simulations are available at https://osf.io/mjgk9/. The design priors were consistent with our planned analysis priors (see \textit{\nameref{statistical_tests}}).}
\end{figure}



